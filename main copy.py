import json
from pathlib import Path
from vllm import LLM, SamplingParams
import torch
from itertools import islice
import argparse
from transformers import AutoTokenizer
from PIL import Image
from utils import load_remaining_records, load_image
from build_prompt import build_prompt

# ========== å‚æ•° ==========
parser = argparse.ArgumentParser()
parser.add_argument("--model_path", type=str, required=True)
parser.add_argument("--input_jsonl", type=str, required=True)
parser.add_argument("--output_jsonl", type=str, required=True)
parser.add_argument("--prompt_name", type=str, required=True,
                    help="æ¨¡æ¿æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„ã€ä¸å«åç¼€ï¼‰")
parser.add_argument("--model_name", type=str, required=True,
                    help="æ¨¡å‹åç§°")
parser.add_argument("--prompt_dir", type=str, default="prompts",
                    help="å­˜æ”¾æ¨¡æ¿æ–‡ä»¶çš„ç›®å½•")
parser.add_argument("--temperature", type=float, default=0.)
parser.add_argument("--top_p", type=float, default=0.95)
parser.add_argument("--max_tokens", type=int, default=4096)
parser.add_argument("--n", type=int, default=1)
parser.add_argument("--chunk_size", type=int, default=32)
parser.add_argument("--start_idx", type=int, default=None, help="èµ·å§‹ indexï¼ˆåŒ…å«ï¼‰")
parser.add_argument("--end_idx", type=int, default=None, help="ç»“æŸ indexï¼ˆä¸åŒ…å«ï¼‰")
parser.add_argument("--save_images", action="store_true", help="æ˜¯å¦ä¿å­˜è¾“å‡ºè®°å½•ä¸­çš„ image å­—æ®µåˆ°æ–‡ä»¶")  # ğŸ†• æ–°å¢å‚æ•°
args = parser.parse_args()

# ========== é…ç½® ==========
model_path = args.model_path
input_jsonl = Path(args.input_jsonl)
output_jsonl = Path(args.output_jsonl)
prompt_file = Path(args.prompt_dir) / f"{args.prompt_name}.txt"
assert prompt_file.exists(), f"âŒ æ‰¾ä¸åˆ°æ¨¡æ¿æ–‡ä»¶: {prompt_file}"

data_parallel_size = torch.cuda.device_count() if torch.cuda.is_available() else 1

# ========== åŠ è½½æ¨¡æ¿ ==========
prompt_template = prompt_file.read_text(encoding="utf-8")
uses_image = "{image_path}" in prompt_template
if uses_image:
    print("ğŸ–¼ï¸ æ¨¡æ¿ä½¿ç”¨ {image_path} ï¼Œå°†åŠ è½½å¯¹åº”å›¾ç‰‡")

# ========== è¯»å–å¾…ç”Ÿæˆè®°å½• ==========
remaining_records = load_remaining_records(
    input_jsonl=input_jsonl,
    output_jsonl=output_jsonl,
    prompt_template=prompt_template,
    use_image_basename=True,
    start_idx=args.start_idx,
    end_idx=args.end_idx,
)
if not remaining_records:
    exit(0)

# ========== åˆå§‹åŒ– LLM ==========
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
llm = LLM(
    model=model_path,
    max_model_len=args.max_tokens + 2048,
    mm_processor_kwargs={
        "min_pixels": 28 * 28,
        "max_pixels": 1024 * 1024,
    },
    data_parallel_size=data_parallel_size
)

sampling_params = SamplingParams(
    n=args.n,
    temperature=args.temperature,
    top_p=args.top_p,
    max_tokens=args.max_tokens,
)

# ========== å·¥å…·å‡½æ•° ==========
def render_prompt(template: str, fields: dict) -> str:
    """ç”¨è¾“å…¥å­—æ®µå¡«å……æ¨¡æ¿"""
    try:
        return template.format(**fields)
    except KeyError as e:
        missing = str(e).strip("'")
        raise KeyError(f"âŒ æ¨¡æ¿ä¸­ä½¿ç”¨äº†æœªæä¾›çš„å­—æ®µ: {missing}")

def chunked_iterable(iterable, size):
    it = iter(iterable)
    while True:
        chunk = list(islice(it, size))
        if not chunk:
            break
        yield chunk

# ========== ä¸»å¾ªç¯ ==========
output_jsonl.parent.mkdir(parents=True, exist_ok=True)

# ğŸ†• å¦‚æœå¼€å¯äº†ä¿å­˜å›¾ç‰‡é€‰é¡¹ï¼Œåˆ™åˆ›å»ºç›®å½•
output_image_dir = None
if args.save_images:
    output_image_dir = output_jsonl.parent / "images"
    output_image_dir.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ’¾ å›¾ç‰‡ä¿å­˜å·²å¼€å¯ï¼Œå°†è¾“å‡ºåˆ°: {output_image_dir}")

total = len(remaining_records)
processed = 0

with output_jsonl.open("a", encoding="utf-8") as fout:
    for chunk in chunked_iterable(remaining_records, args.chunk_size):
        prompts = []
        images = []
        valid_records = []

        for r in chunk:
            try:
                text = render_prompt(prompt_template, r)
                text = build_prompt(text, args.model_name)
                if uses_image:
                    if "image" in r and r["image"] is not None:
                        img = r["image"]
                    else:
                        img_path = Path(r["image_path"])
                        if not img_path.exists():
                            print(f"âš ï¸ å›¾ç‰‡ä¸å­˜åœ¨ï¼Œè·³è¿‡: {img_path}")
                            continue
                        img = load_image(img_path)
                    images.append(img)
                else:
                    images.append(None)
                prompts.append(text)
                valid_records.append(r)
            except Exception as e:
                print(f"âš ï¸ æ„é€  prompt å‡ºé”™ï¼Œè·³è¿‡: {r}\n{e}")
                continue

        if not prompts:
            continue

        try:
            outputs = llm.generate(
                [
                    {
                        "prompt": t,
                        "multi_modal_data": {"image": img} if uses_image else None
                    }
                    for t, img in zip(prompts, images)
                ],
                sampling_params=sampling_params
            )
        except torch.cuda.OutOfMemoryError:
            print("âŒ GPU æ˜¾å­˜ä¸è¶³ï¼Œå»ºè®®å‡å° batch/chunk size æˆ– max_tokens")
            torch.cuda.empty_cache()
            continue
        except Exception as e:
            print(f"âŒ llm.generate() å‡ºé”™ï¼Œæœ¬æ‰¹æ¬¡è·³è¿‡ã€‚\né”™è¯¯è¯¦æƒ…: {repr(e)}")
            continue

        for record, out in zip(valid_records, outputs):
            # ğŸ†• æ ¹æ® save_images å‚æ•°å†³å®šå¤„ç†æ–¹å¼
            if "image" in record:
                if args.save_images and isinstance(record["image"], Image.Image):
                    img_save_path = output_image_dir / f"{record.get('id', processed):06d}.jpg"
                    try:
                        record["image"].save(img_save_path)
                        record["image_path"] = str(img_save_path)
                    except Exception as e:
                        print(f"âš ï¸ ä¿å­˜å›¾ç‰‡å¤±è´¥: {e}")
                # æ— è®ºæ˜¯å¦ä¿å­˜ï¼Œéƒ½ç§»é™¤åŸå§‹ image å¯¹è±¡
                del record["image"]

            results = [g.text.strip() for g in out.outputs]
            out_record = {
                **record,
                "outputs": results,
                "prompt_name": args.prompt_name,
            }
            fout.write(json.dumps(out_record, ensure_ascii=False) + "\n")
            fout.flush()
            processed += 1

        print(f"âœ… å·²å¤„ç† {processed}/{total} æ¡ (chunk size={len(chunk)})")

print(f"ğŸ‰ å…¨éƒ¨ç”Ÿæˆå®Œæˆï¼Œç»“æœå†™å…¥ï¼š{output_jsonl}")
